{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of neural tangent kernel performance\n",
    "\n",
    "Given the pre-generated neural tangent kernel (NTK) output from the main code (by default in the directory `'./kernel_output'`), we examine the classification performance on the MNIST dataset of the exact, sparsified, and diagonal NTKs. Additionally, for the quantum algorithms of sparsified and diagonal NTKs, the condition number and the number of measurements required for post-selection/readout are verified to be bounded by $O(\\log n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "  \n",
    "sns.set(font_scale=1.3)\n",
    "sns.set_style(\"whitegrid\", {\"axes.facecolor\": \".97\"})\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a sparsity pattern is constructed in $\\tilde O(n)$ time. In the proposed quantum algorithm, this is performed once when the data is stored in a binary QRAM data structure (also in $\\tilde O(n)$ time). Given a sparsity pattern with at most $s = O(\\log n)$ nonzero elements in any row or column, multiple neural networks (of different architectures) can be efficiently trained in logarithmic time using the same sparsity pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_sparsity(m):\n",
    "    \"\"\"\n",
    "    Get expected matrix sparsity, chosen to be O(log n).\n",
    "    \"\"\"\n",
    "    return np.log(m.shape[0])\n",
    "\n",
    "def block_diagonal(m):\n",
    "    \"\"\"\n",
    "    Prepare a block diagonal matrix [[1, 0], [0, 1]] corresponding to the two data classes\n",
    "    in the NTK matrix.\n",
    "    \"\"\"\n",
    "    class_size = m.shape[0]//2\n",
    "    ones_class = np.ones((class_size, class_size))\n",
    "    zeros_class = np.zeros((class_size, class_size))\n",
    "    class_0 = np.block([[ones_class, zeros_class], [zeros_class, zeros_class]])\n",
    "    class_1 = np.block([[zeros_class, zeros_class], [zeros_class, ones_class]])\n",
    "    return class_0, class_1\n",
    "\n",
    "def get_sparsity_pattern(m):\n",
    "    \"\"\"\n",
    "    Prepare in O(n log n) time a sparsity pattern over the n x n matrix with a\n",
    "    pseudorandom generator.\n",
    "    \"\"\"\n",
    "    target_sparsity = get_target_sparsity(m)\n",
    "    \n",
    "    # procedure produces an equivalent distribution of 1s and 0s as sampling individual\n",
    "    # matrix elements i.i.d. from binomial distribution\n",
    "    \n",
    "    # since we'll take half of the generated indices, we set the probability of a nonzero\n",
    "    # element to be double the target sparsity\n",
    "    p_one = min(2*target_sparsity/m.shape[0], 1.0)\n",
    "    \n",
    "    # for each row, sample the binomial distribution to get the number of nonzero indices\n",
    "        # matches in expectation get_target_sparsity(m), i.e. O(log n)\n",
    "    # reference the upper triangular indices according to the lower triangular indices\n",
    "        # can be done efficiently by mapping indices instead of copying matrix elements\n",
    "\n",
    "    one_filter = np.zeros(m.shape)\n",
    "    for i in range(m.shape[0]):\n",
    "        # find O(log n) indices\n",
    "        num_nonzero = np.random.randint(m.shape[0],\n",
    "                                        size=np.random.binomial(m.shape[0], p_one))\n",
    "        one_filter[i][num_nonzero] = 1\n",
    "    one_filter = np.tril(one_filter) + np.tril(one_filter, -1).T\n",
    "    \n",
    "    # set all NTK matrix elements from opposite classes to be zero\n",
    "    # since the NTK is larger for more similar data examples, this biases the sparse\n",
    "    # matrix towards selecting more important examples\n",
    "    class_0, class_1 = block_diagonal(m)\n",
    "    one_filter = one_filter * (class_0 + class_1)\n",
    "    \n",
    "    # make sure the diagonal is ones\n",
    "    np.fill_diagonal(one_filter, 1)\n",
    "    \n",
    "    return one_filter\n",
    "\n",
    "def sparsify_unbiased(m, sparsity_pattern):\n",
    "    \"\"\"\n",
    "    Sparsify NTK matrix `m` using a given sparsity pattern.\n",
    "    Used for the fully-connected network.\n",
    "    \"\"\"\n",
    "    return m * sparsity_pattern\n",
    "\n",
    "def sparsify_biased(m, sparsity_pattern, t0, t1):\n",
    "    \"\"\"\n",
    "    Sparsify NTK matrix `m` using a given sparsity pattern, then additionally sparsify by\n",
    "    setting elements below `t0` and `t1` in classes 0 and 1 respectively to 0.\n",
    "    Used for the convolutional network.\n",
    "    \"\"\"\n",
    "    class_0, class_1 = block_diagonal(m)\n",
    "    one_filter = sparsity_pattern * ((m > t0) * class_0 + (m > t1) * class_1)\n",
    "    np.fill_diagonal(one_filter, 1)\n",
    "    \n",
    "    kernel_train_sparse = m * one_filter\n",
    "    \n",
    "    # we expect a factor of ~target_sparsity by Gershgorin's theorem\n",
    "    # empirically, the well-conditioning of the kernel makes it scale better than this\n",
    "    f = 0.76 * get_target_sparsity(m)**0.9\n",
    "    conditioning = f * np.diag(kernel_train_sparse)*np.eye(kernel_train_sparse.shape[0])\n",
    "    kernel_train_conditioned = kernel_train_sparse + conditioning\n",
    "    return kernel_train_conditioned\n",
    "\n",
    "def compute_class_percentiles(m, percentile):\n",
    "    \"\"\"\n",
    "    Compute the truncation thresholds for `sparsify_biased`. This is evaluated over a\n",
    "    small subset (n = 16) of the training set to efficiently bias the sparsification\n",
    "    towards large off-diagonal elements.\n",
    "    \"\"\"\n",
    "    class_size = m.shape[0]//2\n",
    "    ones_class = np.ones((class_size, class_size))\n",
    "    zeros_class = np.zeros((class_size, class_size))\n",
    "    class_0 = np.block([[ones_class - np.eye(class_size), zeros_class],\n",
    "                        [zeros_class, zeros_class]])\n",
    "    class_1 = np.block([[zeros_class, zeros_class],\n",
    "                        [zeros_class, ones_class - np.eye(class_size)]])\n",
    "    t0 = np.percentile(np.abs(m * class_0), percentile)\n",
    "    t1 = np.percentile(np.abs(m * class_1), percentile)\n",
    "    return t0, t1\n",
    "\n",
    "def get_sparsity(m):\n",
    "    \"\"\"\n",
    "    Get maximum number of nonzero elements in any row or column.\n",
    "    \"\"\"\n",
    "    return np.amax(np.sum(m != 0, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the sparsity pattern does indeed scale like $O(\\log n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [16, 32, 64, 128, 256, 512]\n",
    "sparsity_trials = 100\n",
    "sparsities = np.zeros(len(Ns))\n",
    "sparsities_std = np.zeros(len(Ns))\n",
    "for i in range(len(Ns)):\n",
    "    N = Ns[i]\n",
    "    sparsities_N = []\n",
    "    for t in range(sparsity_trials):\n",
    "        sparsity_pattern = get_sparsity_pattern(np.zeros((N, N)))\n",
    "        s = get_sparsity(sparsity_pattern)\n",
    "        sparsities_N.append(s)\n",
    "    sparsities[i] = np.mean(sparsities_N)\n",
    "    sparsities_std[i] = np.std(sparsities_N)/np.sqrt(len(sparsities_N))\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.errorbar(Ns, sparsities, yerr=2*sparsities_std, fmt='o', c='C1')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Sparsity')\n",
    "plt.xscale('log')\n",
    "plt.xticks(Ns)\n",
    "plt.minorticks_off()\n",
    "plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network performance\n",
    "\n",
    "Four quantities characterize the infinite-width neural network and its sparsified and diagonal approximations:\n",
    "* Binary classification accuracy: all three networks are evaluated on a balanced sample of the MNIST test set (separate from the training set).\n",
    "* Condition number: to invert the sparsified NTK $\\tilde K$ efficiently with a quantum linear systems algorithm, the condition number $\\kappa(\\tilde K)$ (defined to be the ratio of the largest to smallest singular values) must be bounded by $O(\\log n)$.\n",
    "* Post-selection measurements: to prepare the quantum state $|k_*\\rangle = \\frac{1}{\\sqrt{P}} \\sum_{i=0}^{n-1} k_i |i\\rangle$ of the NTK evaluated between test data $\\mathbf x_*$ and the training data $\\{\\mathbf x_i\\}$, we require $O(1/P)$ measurements for $P = \\sum_i k_i^2$. Here, $k_i$ corresponds to the kernel $k(\\mathbf x_*, \\mathbf x_i)$ normalized and clipped to lie within $-1 \\leq k_i \\leq 1$. To efficiently prepare the state, the number of measurements must be bounded by $O(\\log n)$.\n",
    "* Readout measurements: to perform the final readout, we estimate the sign of state overlap $o = \\langle k_* | y \\rangle$ (for the diagonal approximation) or $o = \\langle k_* | \\tilde K^{-1} | y\\rangle$ (for the sparsified approximation). This requires $O(1/|o|^2)$ measurements, which must be bounded by $O(\\log n)$ for efficient readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(ntk_mean):\n",
    "    \"\"\"\n",
    "    Classify raw output of the NTK on the test dataset, assuming the test data is sampled\n",
    "    i.i.d. from the underlying data distribution (i.e. balanced).\n",
    "    \"\"\"\n",
    "    thresh = np.median(ntk_mean)\n",
    "    out = np.sign(ntk_mean - thresh)\n",
    "    return out\n",
    "\n",
    "def get_file_prefix(fp, seed, N, trial):\n",
    "    \"\"\"\n",
    "    NTK output filename\n",
    "    \"\"\"\n",
    "    return fp + '_seed' + str(seed) + '_data' + str(N) + '_trial' + str(trial) + '_'\n",
    "\n",
    "def analyze(file_prefix, Ns, sparsify_fnc, sparsify_args=(), sparsity_bootstraps=3,\n",
    "               plot_margin=0):\n",
    "    \"\"\"\n",
    "    Plot the accuracy, condition number, number of measurements for post-selection, and\n",
    "    number of measurements for readout.\n",
    "    \"\"\"\n",
    "    Ns = np.array(Ns)\n",
    "    accs_mean = []\n",
    "    accs_std = []\n",
    "    measurements = []\n",
    "    post_selections = []\n",
    "    measurements_std = []\n",
    "    post_selections_std = []\n",
    "    \n",
    "    all_kappas = []\n",
    "    for n_ind in range(len(Ns)):\n",
    "        N = Ns[n_ind]\n",
    "        \n",
    "        # load data\n",
    "        prefix = get_file_prefix(file_prefix, '*', N, '*')\n",
    "        suffixes = ['kernel_train.npy', 'kernel_test.npy', 'kernel_test_normalized.npy',\n",
    "                    'train_label.npy', 'test_label.npy']\n",
    "        files = []\n",
    "        for suffix in suffixes:\n",
    "            files.append(sorted(glob.glob(prefix + '*' + suffix)))\n",
    "        \n",
    "        all_dense = []\n",
    "        all_sparse = []\n",
    "        all_identity = []\n",
    "        all_scale = []\n",
    "        \n",
    "        trial_p = []\n",
    "        trial_overlaps_diag = []\n",
    "        trial_overlaps_sparse = []\n",
    "        kappas = []\n",
    "        \n",
    "        for i in range(len(files[0])):\n",
    "            # load files\n",
    "            kernel_train = np.load(files[0][i])\n",
    "            kernel_test = np.load(files[1][i])\n",
    "            kernel_test_normalized = np.load(files[2][i])\n",
    "            train_label = np.load(files[3][i])\n",
    "            test_label = np.load(files[4][i])\n",
    "            \n",
    "            # bootstrap over different sparsity patterns\n",
    "            for s in range(sparsity_bootstraps):\n",
    "                # randomize sparsity pattern\n",
    "                sparsity_pattern = get_sparsity_pattern(kernel_train)\n",
    "                \n",
    "                # sparsify kernel\n",
    "                kernel_train_sparse = sparsify_fnc(kernel_train, sparsity_pattern,\n",
    "                                                   *sparsify_args)\n",
    "                kernel_train_identity = np.diag(kernel_train)*np.eye(kernel_train.shape[0])\n",
    "                \n",
    "                # calculate condition number\n",
    "                eigs = np.linalg.eigvals(kernel_train_sparse)\n",
    "                kappa = np.amax(np.abs(eigs))/np.amin(np.abs(eigs))\n",
    "                kappas.append(kappa)\n",
    "                \n",
    "                # solve A^{-1}y for A being the exact NTK, sparsified NTK, and diagonal NTK\n",
    "                inv_y_dense = np.linalg.inv(kernel_train) @ train_label\n",
    "                inv_y_dense /= np.sqrt(np.sum(inv_y_dense**2))\n",
    "                inv_y_sparse = np.linalg.inv(kernel_train_sparse) @ train_label\n",
    "                inv_y_sparse /= np.sqrt(np.sum(inv_y_sparse**2))\n",
    "                inv_y_diag = np.linalg.inv(kernel_train_identity) @ train_label\n",
    "                inv_y_diag /= np.sqrt(np.sum(inv_y_diag**2))\n",
    "                \n",
    "                # prepare |k_*> state\n",
    "                ki = kernel_test_normalized / np.amax(np.abs(kernel_test_normalized))\n",
    "                p = np.sum(ki**2, axis=1)\n",
    "                ki = ki / np.sqrt(p[:, np.newaxis])\n",
    "                \n",
    "                # prepare |y> state\n",
    "                ny = len(train_label)\n",
    "                y = train_label / np.sqrt(ny)\n",
    "\n",
    "                trial_p.append(p)  # for post-selection measurements\n",
    "                trial_overlaps_diag.append(ki @ y)  # <k_*|y>\n",
    "                trial_overlaps_sparse.append(ki @ inv_y_sparse)  # <k_*|\\tilde K^{-1}|y>\n",
    "\n",
    "                # classify with the exact, sparsified, and diagonal NTKs\n",
    "                mean_dense = kernel_test @ inv_y_dense\n",
    "                mean_sparse = kernel_test_normalized @ inv_y_sparse\n",
    "                mean_identity = kernel_test_normalized @ inv_y_diag\n",
    "                correct_dense = classify(mean_dense) == test_label\n",
    "                correct_sparse = classify(mean_sparse) == test_label\n",
    "                correct_identity = classify(mean_identity) == test_label\n",
    "\n",
    "                all_dense = np.concatenate((all_dense, correct_dense))\n",
    "                all_sparse = np.concatenate((all_sparse, correct_sparse))\n",
    "                all_identity = np.concatenate((all_identity, correct_identity))\n",
    "                all_scale.append([trial_p, trial_overlaps_diag, trial_overlaps_sparse])\n",
    "        \n",
    "        # compute the mean and standard deviation of all quantities\n",
    "        \n",
    "        all_out = [all_dense, all_sparse, all_identity]\n",
    "        accs_mean_s = []\n",
    "        accs_std_s = []\n",
    "        for i in range(len(all_out)):\n",
    "            correct = all_out[i]\n",
    "            accs_mean_s.append(np.mean(correct))\n",
    "            accs_std_s.append(np.std(correct)/np.sqrt(len(correct)))\n",
    "        accs_mean.append(accs_mean_s)\n",
    "        accs_std.append(accs_std_s)\n",
    "        \n",
    "        scale = np.concatenate(all_scale, axis=1)\n",
    "        p = scale[0, :, :].flatten()\n",
    "        post_measurements = N/p\n",
    "        post_selections.append(np.median(post_measurements))\n",
    "        bootstraps = 5 # Poisson bootstrapping\n",
    "        medians = np.zeros(bootstraps)\n",
    "        for b in range(bootstraps):\n",
    "            r = np.random.poisson(size=post_measurements.shape)\n",
    "            pm = r * post_measurements\n",
    "            medians[b] = np.median(pm)\n",
    "        post_selections_std.append(np.std(medians)/np.sqrt(bootstraps))\n",
    "        \n",
    "        overlaps = scale[1:, :, :].reshape(2, -1)\n",
    "        # enough measurements for stdev to be O(overlap)\n",
    "        these_measurements = 1/overlaps**2 - 1\n",
    "        measurements.append(np.median(these_measurements, axis=1))\n",
    "        \n",
    "        bootstraps = 5 # Poisson bootstrapping\n",
    "        medians = np.zeros((bootstraps, 2))\n",
    "        for b in range(bootstraps):\n",
    "            r = np.random.poisson(size=these_measurements.shape)\n",
    "            pm = r * these_measurements\n",
    "            medians[b] = np.median(pm, axis=1)\n",
    "        measurements_std.append(np.std(medians, axis=0)/np.sqrt(bootstraps))\n",
    "        all_kappas.append(kappas)\n",
    "        \n",
    "    accs_mean = np.array(accs_mean)\n",
    "    accs_std = np.array(accs_std)\n",
    "    \n",
    "    post_selections = (np.array(post_selections), np.array(post_selections_std))\n",
    "    measurements = (np.array(measurements), np.array(measurements_std))\n",
    "    \n",
    "    kappa = []\n",
    "    kappa_std = []\n",
    "    for row in all_kappas:\n",
    "        kappa.append(np.mean(row))\n",
    "        kappa_std.append(np.std(row)/np.sqrt(len(row)))\n",
    "    kappa = np.array(kappa)\n",
    "    kappa_std = np.array(kappa_std)\n",
    "    \n",
    "    # plot everything\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.errorbar(Ns - Ns*plot_margin, accs_mean[:, 0], yerr=2*accs_std[:, 0],\n",
    "                 label='Exact NTK', fmt='o')\n",
    "    plt.errorbar(Ns, accs_mean[:, 1], yerr=2*accs_std[:, 1], label='Sparse NTK', fmt='o')\n",
    "    plt.errorbar(Ns + Ns*plot_margin, accs_mean[:, 2], yerr=2*accs_std[:, 2],\n",
    "                 label='Diagonal NTK', fmt='o')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xscale('log')\n",
    "    plt.xticks(Ns)\n",
    "    plt.minorticks_off()\n",
    "    plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.errorbar(Ns, kappa, yerr=2*kappa_std, fmt='o', c='C1')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Condition number')\n",
    "    plt.xscale('log')\n",
    "    plt.xticks(Ns)\n",
    "    plt.minorticks_off()\n",
    "    plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.gca().get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter(\n",
    "                                                                         useOffset=False))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.errorbar(Ns, post_selections[0], yerr=2*post_selections[1], fmt='o')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Measurements (post-selection)')\n",
    "    plt.xscale('log')\n",
    "    plt.xticks(Ns)\n",
    "    plt.minorticks_off()\n",
    "    plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.errorbar(Ns - Ns*plot_margin/2, measurements[0][:, 1],\n",
    "                 yerr=2*measurements[1][:, 1], label='Sparse NTK', c='C1', fmt='o')\n",
    "    plt.errorbar(Ns + Ns*plot_margin/2, measurements[0][:, 0],\n",
    "                 yerr=2*measurements[1][:, 0], label='Diagonal NTK', c='C2', fmt='o')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Measurements (readout)')\n",
    "    plt.xscale('log')\n",
    "    plt.xticks(Ns)\n",
    "    plt.minorticks_off()\n",
    "    plt.gca().get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results for the fully-connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze('kernel_output/fully-connected', Ns, sparsify_unbiased, plot_margin=1/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the appropriate normalization threshold for preparing $|k_*\\rangle$ based on a small subset ($n=16$) of the training set, and then plot the results for the convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'kernel_output/convolutional'\n",
    "base_n = 16\n",
    "base_ntk = np.load(sorted(glob.glob(get_file_prefix(fp, '*', base_n, '*') + 'kernel_train.npy'))[0])\n",
    "sparsify_args = compute_class_percentiles(base_ntk, 90)\n",
    "analyze(fp, Ns, sparsify_biased, sparsify_args=sparsify_args, plot_margin=1/8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
